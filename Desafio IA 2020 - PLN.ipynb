{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio IA 2020 - PLN\n",
    "## Serpro\n",
    "\n",
    "### Participante: Andréa Jesus dos Santos\n",
    "### Equipe: Leão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Deinha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics #for accuracy calculation\n",
    "#import os\n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import emocoes_texto_classificacao\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as skldata\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregar Dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim do carregamento de dados\n"
     ]
    }
   ],
   "source": [
    "caminhoDatabase = \"C:\\\\Users\\\\Deinha\\\\Documents\\\\Serpro\\\\DesafioIA\\\\PLN\\\\Codigo\\\\Dataset\\\\\"\n",
    "\n",
    "TREINO_PATH = caminhoDatabase + \"treino.csv\"\n",
    "TESTE_PATH = caminhoDatabase + \"teste-sem-classe.csv\"\n",
    "\n",
    "#Dados de Treino\n",
    "#treino_data = skldata.load_files(TREINO_PATH, shuffle=False, load_content=True, encoding=\"utf-8\")\n",
    "\n",
    "dfTreino_original = pd.read_csv(TREINO_PATH, sep=\",\", encoding='utf-8')\n",
    "#nomes_classificacao_treino = treino_data.target_names\n",
    "#label2idx = {label: idx for idx, label in enumerate(nomes_classificacao_treino)}\n",
    "\n",
    "#Tipos de Classificação do Texto\n",
    "'''\n",
    "print(\"Nome classificação: \", nomes_classificacao_treino)\n",
    "print(\"Classificação: \", label2idx)\n",
    "print(\"Treino target: \", treino_data.target)\n",
    "'''\n",
    "\n",
    "#Dados de Teste\n",
    "#teste_data = skldata.load_files(TESTE_PATH, shuffle=False, load_content=True, encoding=\"utf-8\")\n",
    "dfTeste_original = pd.read_csv(TESTE_PATH, sep=\",\", encoding='utf-8')\n",
    "\n",
    "#Nomes das Colunas\n",
    "#rotulos_colunas = dfTreino_original.columns[0:3]\n",
    "#print(rotulos_colunas)\n",
    "\n",
    "#path = \"../dataset/\"\n",
    "#df = emocoes_texto_classificacao.format_dataset(path)\n",
    "#df.head()\n",
    "\n",
    "print(\"Fim do carregamento de dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=-=-Tratada=-=-\n",
      "   id                                              texto    classe\n",
      "0   0  lul diz sen maior resolv problem presid diz de...    neutro\n",
      "1   1  adolesc mort ouv music alt est unid adolesc  a...  tristeza\n",
      "2   2  core sul insinu hack lig core nort atac intern...    neutro\n",
      "3   3  mama foc sel filhot recemnasc belg filhot nasc...   alegria\n",
      "4   4  adolesc  ano sum ach mort manau segund polic v...  tristeza\n",
      "5   5  bomb localiz corp jov  ano rio mat gross cas r...  tristeza\n",
      "6   6  medic tent explic orig caibr noturn episodi co...  surpresa\n",
      "7   7  govern anunc liberaca r  bilho agricult famili...   alegria\n",
      "8   8  sindicat cuban celebr  ano revoluca dia trabal...    neutro\n",
      "9   9  eclips ajud cient identific assinat planet ter...   alegria\n",
      "Fim do pré processamento\n"
     ]
    }
   ],
   "source": [
    "#Exclusão em memória do campo ID\n",
    "#dfTreino = dfTreino_original.drop(labels=[\"id\"], axis=1, inplace=False)\n",
    "#dfTeste = dfTeste_original.drop(labels=[\"id\"], axis=1, inplace=False)\n",
    "\n",
    "dfTreino = dfTreino_original\n",
    "dfTeste = dfTeste_original\n",
    "\n",
    "#padronozação para letras em minúsculo\n",
    "#remoção de stopwords, caracteres especiais, números e radicais\n",
    "dfTreino_tratado = emocoes_texto_classificacao.text_preprocess(dfTreino)\n",
    "dfTeste_tratado = emocoes_texto_classificacao.text_preprocess(dfTeste, \"\")\n",
    "\n",
    "print(\"=-=-Tratada=-=-\")\n",
    "print(dfTreino_tratado.head(10))\n",
    "\n",
    "print(\"Fim do pré processamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Seperar dados de treino e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dataset de treino: 1360\n",
      "Tamanho do dataset de teste: 340\n",
      "Fim da separação de dados\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dfTreino_tratado['texto'].values, \n",
    "                                                    dfTreino_tratado['classe'].values, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=10)\n",
    "\n",
    "print('Tamanho do dataset de treino: {}'.format(len(y_train)))\n",
    "print('Tamanho do dataset de teste: {}'.format(len(y_test)))\n",
    "\n",
    "print(\"Fim da separação de dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 100)\n",
      "(340, 100)\n",
      "Fim do text encoding\n"
     ]
    }
   ],
   "source": [
    "stopword = emocoes_texto_classificacao.get_stopwords()\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=(1,2),\n",
    "                        stop_words=stopword,\n",
    "                        max_df=0.8,\n",
    "                        max_features=100)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(x_train)\n",
    "labels_train = y_train\n",
    "\n",
    "features_test = tfidf.transform(x_test)\n",
    "labels_test = y_test\n",
    "\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "\n",
    "print(\"Fim do text encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início do modelo\n",
      "Fim do modelo SVM\n",
      "Fim do modelos GridSearch\n",
      "Fim do modelos Grid Search Fit\n",
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8442 tasks      | elapsed: 36.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9000 out of 9000 | elapsed: 39.1min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiper parametros:\n",
      "{'C': 10, 'degree': 1, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}\n",
      "Acurácia:\n",
      "0.4852941176470588\n",
      "Fim do modelo\n"
     ]
    }
   ],
   "source": [
    "print(\"Início do modelo\")\n",
    "C = [.001, .01, .1, 1, 10]\n",
    "degree = [1, 2, 3, 4, 5]\n",
    "gamma = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "probability = [True, False]\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "param_grid = {'C': C, 'kernel':kernels, 'probability':probability, 'gamma':gamma, 'degree':degree}\n",
    "\n",
    "print(\"Fim do modelo SVM\")\n",
    "svc = svm.SVC(random_state=8)\n",
    "\n",
    "print(\"Fim do modelos GridSearch\")\n",
    "grid_search = GridSearchCV(estimator=svc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=10,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "print(\"Fim do modelos Grid Search Fit\")\n",
    "grid_search.fit(features_train, labels_train)\n",
    "\n",
    "print(\"Melhores hiper parametros:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Acurácia:\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "\n",
    "print(\"Fim do modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste\n",
      "Acurácia: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     alegria       0.00      0.00      0.00        32\n",
      "    desgosto       0.38      0.18      0.24        33\n",
      "        medo       1.00      0.08      0.14        52\n",
      "      neutro       0.43      0.96      0.59        91\n",
      "       raiva       0.00      0.00      0.00        14\n",
      "    surpresa       0.56      0.21      0.30        48\n",
      "    tristeza       0.47      0.66      0.55        70\n",
      "\n",
      "   micro avg       0.45      0.45      0.45       340\n",
      "   macro avg       0.40      0.30      0.26       340\n",
      "weighted avg       0.48      0.45      0.36       340\n",
      "\n",
      "[[0 0]\n",
      " [0 0]]\n",
      "Fim do resultados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Teste\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(features_train, labels_train)\n",
    "\n",
    "y_predict = best_model.predict(features_test)\n",
    "print(\"Acurácia: {}\".format(accuracy_score(labels_test, y_predict)))\n",
    "print(classification_report(labels_test,y_predict))\n",
    "print(confusion_matrix(labels_test, y_predict, labels=['alegria', 'raiva']))\n",
    "\n",
    "#TODO: Gerar a tabela com o resultado alcançado\n",
    "\n",
    "print(\"Fim do resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Extras - com parâmetros pré-definidos\n",
    "\n",
    "Para rodar esses resultados, é necessário setar o parametro ind_stemmer em notebooks/emocoes_texto_classificacao.py como 1 (com stemmer) ou 0 (sem stemmer) e rodar novamente o código\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com stemmer\n",
      "C=0.3\n",
      "Acurácia: 0.45588235294117646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     alegria       0.00      0.00      0.00        32\n",
      "    desgosto       0.36      0.24      0.29        33\n",
      "        medo       0.64      0.13      0.22        52\n",
      "      neutro       0.44      0.92      0.60        91\n",
      "       raiva       0.00      0.00      0.00        14\n",
      "    surpresa       0.48      0.25      0.33        48\n",
      "    tristeza       0.49      0.63      0.55        70\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       340\n",
      "   macro avg       0.34      0.31      0.28       340\n",
      "weighted avg       0.42      0.46      0.38       340\n",
      "\n",
      "[[0 0]\n",
      " [0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C=1.0\n",
      "Acurácia: 0.4852941176470588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     alegria       0.14      0.03      0.05        32\n",
      "    desgosto       0.36      0.36      0.36        33\n",
      "        medo       0.54      0.29      0.37        52\n",
      "      neutro       0.51      0.87      0.64        91\n",
      "       raiva       0.75      0.21      0.33        14\n",
      "    surpresa       0.44      0.29      0.35        48\n",
      "    tristeza       0.51      0.59      0.55        70\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       340\n",
      "   macro avg       0.46      0.38      0.38       340\n",
      "weighted avg       0.46      0.49      0.44       340\n",
      "\n",
      "[[1 0]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "print('Com stemmer')\n",
    "best_model = svm.SVC(kernel='linear', C=.3, probability=True)\n",
    "best_model.fit(features_train, labels_train)\n",
    "y_predict = best_model.predict(features_test)\n",
    "\n",
    "print('C=0.3')\n",
    "print(\"Acurácia: {}\".format(accuracy_score(labels_test, y_predict)))\n",
    "print(classification_report(labels_test,y_predict))\n",
    "print(confusion_matrix(labels_test, y_predict, labels=['alegria', 'raiva']))\n",
    "\n",
    "best_model = svm.SVC(kernel='linear', C=1.0, probability=True)\n",
    "best_model.fit(features_train, labels_train)\n",
    "y_predict = best_model.predict(features_test)\n",
    "\n",
    "print('\\nC=1.0')\n",
    "print(\"Acurácia: {}\".format(accuracy_score(labels_test, y_predict)))\n",
    "print(classification_report(labels_test,y_predict))\n",
    "print(confusion_matrix(labels_test, y_predict, labels=['alegria', 'raiva']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
